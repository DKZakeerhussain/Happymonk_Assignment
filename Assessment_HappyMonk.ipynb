{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ae2ef9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import confusion_matrix, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4aadd1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Activation Functions and loss function as Categorical cross entropy and one hot encoding\n",
    "\n",
    "#1.Ada-Act Activation function g(x) = k0+k1*x and its derivative d(g(x))/dx = k1\n",
    "def ada_act(x,k0,k1):\n",
    "    return k0+k1*x\n",
    "\n",
    "def ada_act_der(Z):\n",
    "    return k1\n",
    "\n",
    "#2.softmax activation Function\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "#3.Loss function \n",
    "def categorical_crossentropy(y_true,y_pred):\n",
    "    return -np.sum(y_true*np.log(y_pred+1e-10))/y_true.shape[0]\n",
    "\n",
    "#4.one hot encoding\n",
    "def one_hot_encoding(y):\n",
    "    n_classes = np.max(y)+1\n",
    "    return np.eye(n_classes)[y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bd903d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining our assumed MLP architecture with two hidden layers\n",
    "class MLP:\n",
    "    def __init__(self, input_dim, hidden_dim1, hidden_dim2, output_dim, k0_h1, k1_h1, k0_h2, k1_h2):\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim1 = hidden_dim1\n",
    "        self.hidden_dim2 = hidden_dim2\n",
    "        self.output_dim = output_dim\n",
    "        self.k0_h1 = k0_h1\n",
    "        self.k1_h1 = k1_h1\n",
    "        self.k0_h2 = k0_h2\n",
    "        self.k1_h2 = k1_h2\n",
    "\n",
    "        # Initialize weights and biases for each layer and assuming the stdandard normal distribution\n",
    "        self.w1 = np.random.randn(input_dim, hidden_dim1)\n",
    "        self.b1 = np.random.randn(hidden_dim1)\n",
    "        self.w2 = np.random.randn(hidden_dim1, hidden_dim2)\n",
    "        self.b2 = np.random.randn(hidden_dim2)\n",
    "        self.w3 = np.random.randn(hidden_dim2, output_dim)\n",
    "        self.b3 = np.random.randn(output_dim)\n",
    "         \n",
    "        #Forward Propagation of network\n",
    "    def forward_propagation(self, X):\n",
    "        z1 = np.dot(X, self.w1) + self.b1\n",
    "        a1 = ada_act(z1, self.k0_h1, self.k1_h1)\n",
    "\n",
    "        z2 = np.dot(a1, self.w2) + self.b2\n",
    "        a2 = ada_act(z2, self.k0_h2, self.k1_h2)\n",
    "\n",
    "        z3 = np.dot(a2, self.w3) + self.b3\n",
    "        a3 = softmax(z3)\n",
    "\n",
    "        return a1, a2, a3\n",
    "    \n",
    "        \n",
    "        #Backward propagtaion of network\n",
    "    def backward_propagation(self, X, y, a1, a2, a3, learning_rate):\n",
    "        m = X.shape[0]\n",
    "\n",
    "        # Compute gradients for the output layer\n",
    "        dz3 = a3 - y\n",
    "        dw3 = np.dot(a2.T, dz3) / m\n",
    "        db3 = np.mean(dz3, axis=0) \n",
    "\n",
    "        # Compute gradients for the second hidden layer\n",
    "        da2 = np.dot(dz3, self.w3.T)\n",
    "        dz2 = da2 * self.k1_h2\n",
    "        dw2 = np.dot(a1.T, dz2) / m\n",
    "        db2 = np.mean(dz2, axis=0) \n",
    "\n",
    "        # Compute gradients for the first hidden layer\n",
    "        da1 = np.dot(dz2, self.w2.T)\n",
    "        dz1 = da1 * self.k1_h1\n",
    "        dw1 = np.dot(X.T, dz1) / m\n",
    "        db1 = np.mean(dz1, axis=0) \n",
    "\n",
    "        # Update weights and biases using gradient descent\n",
    "        self.w1 -= learning_rate * dw1\n",
    "        self.b1 -= learning_rate * db1\n",
    "        self.w2 -= learning_rate * dw2\n",
    "        self.b2 -= learning_rate * db2\n",
    "        self.w3 -= learning_rate * dw3\n",
    "        self.b3 -= learning_rate * db3\n",
    "        \n",
    "        \n",
    "        #Training of neural network\n",
    "    def train(self, X_train, y_train, X_test, y_test, num_epochs, learning_rate):\n",
    "        train_losses = []\n",
    "        test_losses = []\n",
    "        train_accs = []\n",
    "        test_accs = []\n",
    "        weight_updates = []\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            # Forward propagation\n",
    "            a1, a2, a3 = self.forward_propagation(X_train)\n",
    "\n",
    "            # Compute loss and accuracy for training set\n",
    "            train_loss = categorical_crossentropy(y_train, a3)\n",
    "            train_losses.append(train_loss)\n",
    "            train_pred_labels = np.argmax(a3, axis=1)\n",
    "            train_true_labels = np.argmax(y_train, axis=1)\n",
    "            train_acc = accuracy_score(train_true_labels, train_pred_labels)\n",
    "            train_accs.append(train_acc)\n",
    "\n",
    "            # Backward propagation and weight updates\n",
    "            self.backward_propagation(X_train, y_train, a1, a2, a3, learning_rate)\n",
    "\n",
    "            # Compute loss and accuracy for test set\n",
    "            a1_test, a2_test, a3_test = self.forward_propagation(X_test)\n",
    "            test_loss = categorical_crossentropy(y_test, a3_test)\n",
    "            test_losses.append(test_loss)\n",
    "            test_pred_labels = np.argmax(a3_test, axis=1)\n",
    "            test_true_labels = np.argmax(y_test, axis=1)\n",
    "            test_acc = accuracy_score(test_true_labels, test_pred_labels)\n",
    "            test_accs.append(test_acc)\n",
    "            \n",
    "            \n",
    "            # Save the weight updates for each layer\n",
    "            weight_updates.append((np.copy(self.w1), np.copy(self.b1), np.copy(self.w2), np.copy(self.b2), np.copy(self.w3), np.copy(self.b3)))\n",
    "            print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}\")\n",
    "\n",
    "        return train_losses, test_losses, train_accs, test_accs,weight_updates\n",
    "    \n",
    "    \n",
    "    def predict(self, X):\n",
    "        _, _, predictions = self.forward_propagation(X)\n",
    "        return np.argmax(predictions, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7413b0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ba79d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#one hot encoding\n",
    "y_encoded = one_hot_encoding(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffac9c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e02cc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the input features (optional but recommended)\n",
    "X_train = (X_train - X_train.mean(axis=0)) / X_train.std(axis=0)\n",
    "X_test = (X_test - X_test.mean(axis=0)) / X_test.std(axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "913d4158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the MLP with the specified architecture and parameters\n",
    "input_dim = X_train.shape[1]\n",
    "hidden_dim1 = 16\n",
    "hidden_dim2 = 8\n",
    "output_dim = y_train.shape[1]  # Number of classes\n",
    "k0_h1, k1_h1 = np.random.randn(), np.random.randn()\n",
    "k0_h2, k1_h2 = np.random.randn(), np.random.randn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c130d59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Train Loss: 8.4827, Test Loss: 8.9482, Train Acc: 0.5083, Test Acc: 0.4667\n",
      "Epoch 2/100, Train Loss: 8.1236, Test Loss: 8.5586, Train Acc: 0.5083, Test Acc: 0.4333\n",
      "Epoch 3/100, Train Loss: 8.0801, Test Loss: 4.9950, Train Acc: 0.4083, Test Acc: 0.4333\n",
      "Epoch 4/100, Train Loss: 5.0570, Test Loss: 1.5439, Train Acc: 0.4250, Test Acc: 0.6667\n",
      "Epoch 5/100, Train Loss: 1.5262, Test Loss: 0.2851, Train Acc: 0.6167, Test Acc: 0.8333\n",
      "Epoch 6/100, Train Loss: 0.3315, Test Loss: 0.1120, Train Acc: 0.9083, Test Acc: 1.0000\n",
      "Epoch 7/100, Train Loss: 0.1790, Test Loss: 0.0760, Train Acc: 0.9333, Test Acc: 1.0000\n",
      "Epoch 8/100, Train Loss: 0.1328, Test Loss: 0.0651, Train Acc: 0.9500, Test Acc: 1.0000\n",
      "Epoch 9/100, Train Loss: 0.1146, Test Loss: 0.0603, Train Acc: 0.9583, Test Acc: 1.0000\n",
      "Epoch 10/100, Train Loss: 0.1073, Test Loss: 0.0579, Train Acc: 0.9583, Test Acc: 1.0000\n",
      "Epoch 11/100, Train Loss: 0.1037, Test Loss: 0.0565, Train Acc: 0.9583, Test Acc: 1.0000\n",
      "Epoch 12/100, Train Loss: 0.1015, Test Loss: 0.0556, Train Acc: 0.9667, Test Acc: 1.0000\n",
      "Epoch 13/100, Train Loss: 0.1000, Test Loss: 0.0551, Train Acc: 0.9667, Test Acc: 1.0000\n",
      "Epoch 14/100, Train Loss: 0.0989, Test Loss: 0.0548, Train Acc: 0.9667, Test Acc: 1.0000\n",
      "Epoch 15/100, Train Loss: 0.0979, Test Loss: 0.0546, Train Acc: 0.9667, Test Acc: 1.0000\n",
      "Epoch 16/100, Train Loss: 0.0970, Test Loss: 0.0545, Train Acc: 0.9667, Test Acc: 1.0000\n",
      "Epoch 17/100, Train Loss: 0.0962, Test Loss: 0.0544, Train Acc: 0.9667, Test Acc: 1.0000\n",
      "Epoch 18/100, Train Loss: 0.0955, Test Loss: 0.0544, Train Acc: 0.9667, Test Acc: 1.0000\n",
      "Epoch 19/100, Train Loss: 0.0948, Test Loss: 0.0544, Train Acc: 0.9667, Test Acc: 1.0000\n",
      "Epoch 20/100, Train Loss: 0.0941, Test Loss: 0.0544, Train Acc: 0.9667, Test Acc: 1.0000\n",
      "Epoch 21/100, Train Loss: 0.0935, Test Loss: 0.0545, Train Acc: 0.9667, Test Acc: 1.0000\n",
      "Epoch 22/100, Train Loss: 0.0929, Test Loss: 0.0545, Train Acc: 0.9667, Test Acc: 1.0000\n",
      "Epoch 23/100, Train Loss: 0.0923, Test Loss: 0.0546, Train Acc: 0.9667, Test Acc: 1.0000\n",
      "Epoch 24/100, Train Loss: 0.0917, Test Loss: 0.0547, Train Acc: 0.9667, Test Acc: 1.0000\n",
      "Epoch 25/100, Train Loss: 0.0912, Test Loss: 0.0548, Train Acc: 0.9667, Test Acc: 1.0000\n",
      "Epoch 26/100, Train Loss: 0.0906, Test Loss: 0.0549, Train Acc: 0.9667, Test Acc: 1.0000\n",
      "Epoch 27/100, Train Loss: 0.0901, Test Loss: 0.0551, Train Acc: 0.9667, Test Acc: 1.0000\n",
      "Epoch 28/100, Train Loss: 0.0896, Test Loss: 0.0552, Train Acc: 0.9667, Test Acc: 1.0000\n",
      "Epoch 29/100, Train Loss: 0.0891, Test Loss: 0.0553, Train Acc: 0.9667, Test Acc: 1.0000\n",
      "Epoch 30/100, Train Loss: 0.0886, Test Loss: 0.0555, Train Acc: 0.9667, Test Acc: 1.0000\n",
      "Epoch 31/100, Train Loss: 0.0882, Test Loss: 0.0556, Train Acc: 0.9667, Test Acc: 1.0000\n",
      "Epoch 32/100, Train Loss: 0.0877, Test Loss: 0.0558, Train Acc: 0.9667, Test Acc: 1.0000\n",
      "Epoch 33/100, Train Loss: 0.0873, Test Loss: 0.0559, Train Acc: 0.9667, Test Acc: 1.0000\n",
      "Epoch 34/100, Train Loss: 0.0869, Test Loss: 0.0561, Train Acc: 0.9750, Test Acc: 1.0000\n",
      "Epoch 35/100, Train Loss: 0.0865, Test Loss: 0.0563, Train Acc: 0.9750, Test Acc: 1.0000\n",
      "Epoch 36/100, Train Loss: 0.0861, Test Loss: 0.0564, Train Acc: 0.9750, Test Acc: 1.0000\n",
      "Epoch 37/100, Train Loss: 0.0857, Test Loss: 0.0566, Train Acc: 0.9750, Test Acc: 1.0000\n",
      "Epoch 38/100, Train Loss: 0.0853, Test Loss: 0.0568, Train Acc: 0.9750, Test Acc: 1.0000\n",
      "Epoch 39/100, Train Loss: 0.0849, Test Loss: 0.0570, Train Acc: 0.9750, Test Acc: 1.0000\n",
      "Epoch 40/100, Train Loss: 0.0846, Test Loss: 0.0571, Train Acc: 0.9750, Test Acc: 1.0000\n",
      "Epoch 41/100, Train Loss: 0.0842, Test Loss: 0.0573, Train Acc: 0.9750, Test Acc: 0.9667\n",
      "Epoch 42/100, Train Loss: 0.0839, Test Loss: 0.0575, Train Acc: 0.9750, Test Acc: 0.9667\n",
      "Epoch 43/100, Train Loss: 0.0836, Test Loss: 0.0577, Train Acc: 0.9750, Test Acc: 0.9667\n",
      "Epoch 44/100, Train Loss: 0.0833, Test Loss: 0.0579, Train Acc: 0.9750, Test Acc: 0.9667\n",
      "Epoch 45/100, Train Loss: 0.0829, Test Loss: 0.0581, Train Acc: 0.9750, Test Acc: 0.9667\n",
      "Epoch 46/100, Train Loss: 0.0826, Test Loss: 0.0582, Train Acc: 0.9750, Test Acc: 0.9667\n",
      "Epoch 47/100, Train Loss: 0.0824, Test Loss: 0.0584, Train Acc: 0.9750, Test Acc: 0.9667\n",
      "Epoch 48/100, Train Loss: 0.0821, Test Loss: 0.0586, Train Acc: 0.9750, Test Acc: 0.9667\n",
      "Epoch 49/100, Train Loss: 0.0818, Test Loss: 0.0588, Train Acc: 0.9750, Test Acc: 0.9667\n",
      "Epoch 50/100, Train Loss: 0.0815, Test Loss: 0.0590, Train Acc: 0.9750, Test Acc: 0.9667\n",
      "Epoch 51/100, Train Loss: 0.0813, Test Loss: 0.0592, Train Acc: 0.9750, Test Acc: 0.9667\n",
      "Epoch 52/100, Train Loss: 0.0810, Test Loss: 0.0593, Train Acc: 0.9750, Test Acc: 0.9667\n",
      "Epoch 53/100, Train Loss: 0.0807, Test Loss: 0.0595, Train Acc: 0.9750, Test Acc: 0.9667\n",
      "Epoch 54/100, Train Loss: 0.0805, Test Loss: 0.0597, Train Acc: 0.9750, Test Acc: 0.9667\n",
      "Epoch 55/100, Train Loss: 0.0803, Test Loss: 0.0599, Train Acc: 0.9750, Test Acc: 0.9667\n",
      "Epoch 56/100, Train Loss: 0.0800, Test Loss: 0.0601, Train Acc: 0.9750, Test Acc: 0.9667\n",
      "Epoch 57/100, Train Loss: 0.0798, Test Loss: 0.0603, Train Acc: 0.9750, Test Acc: 0.9667\n",
      "Epoch 58/100, Train Loss: 0.0796, Test Loss: 0.0604, Train Acc: 0.9750, Test Acc: 0.9667\n",
      "Epoch 59/100, Train Loss: 0.0794, Test Loss: 0.0606, Train Acc: 0.9750, Test Acc: 0.9667\n",
      "Epoch 60/100, Train Loss: 0.0791, Test Loss: 0.0608, Train Acc: 0.9750, Test Acc: 0.9667\n",
      "Epoch 61/100, Train Loss: 0.0789, Test Loss: 0.0610, Train Acc: 0.9750, Test Acc: 0.9667\n",
      "Epoch 62/100, Train Loss: 0.0787, Test Loss: 0.0612, Train Acc: 0.9750, Test Acc: 0.9667\n",
      "Epoch 63/100, Train Loss: 0.0785, Test Loss: 0.0613, Train Acc: 0.9750, Test Acc: 0.9667\n",
      "Epoch 64/100, Train Loss: 0.0783, Test Loss: 0.0615, Train Acc: 0.9750, Test Acc: 0.9667\n",
      "Epoch 65/100, Train Loss: 0.0781, Test Loss: 0.0617, Train Acc: 0.9750, Test Acc: 0.9667\n",
      "Epoch 66/100, Train Loss: 0.0779, Test Loss: 0.0619, Train Acc: 0.9750, Test Acc: 0.9667\n",
      "Epoch 67/100, Train Loss: 0.0778, Test Loss: 0.0620, Train Acc: 0.9750, Test Acc: 0.9667\n",
      "Epoch 68/100, Train Loss: 0.0776, Test Loss: 0.0622, Train Acc: 0.9750, Test Acc: 0.9667\n",
      "Epoch 69/100, Train Loss: 0.0774, Test Loss: 0.0624, Train Acc: 0.9750, Test Acc: 0.9667\n",
      "Epoch 70/100, Train Loss: 0.0772, Test Loss: 0.0625, Train Acc: 0.9750, Test Acc: 0.9667\n",
      "Epoch 71/100, Train Loss: 0.0771, Test Loss: 0.0627, Train Acc: 0.9750, Test Acc: 0.9667\n",
      "Epoch 72/100, Train Loss: 0.0769, Test Loss: 0.0629, Train Acc: 0.9750, Test Acc: 0.9667\n",
      "Epoch 73/100, Train Loss: 0.0767, Test Loss: 0.0630, Train Acc: 0.9750, Test Acc: 0.9667\n",
      "Epoch 74/100, Train Loss: 0.0766, Test Loss: 0.0632, Train Acc: 0.9750, Test Acc: 0.9667\n",
      "Epoch 75/100, Train Loss: 0.0764, Test Loss: 0.0634, Train Acc: 0.9750, Test Acc: 0.9667\n",
      "Epoch 76/100, Train Loss: 0.0763, Test Loss: 0.0635, Train Acc: 0.9750, Test Acc: 0.9667\n",
      "Epoch 77/100, Train Loss: 0.0761, Test Loss: 0.0637, Train Acc: 0.9750, Test Acc: 0.9667\n",
      "Epoch 78/100, Train Loss: 0.0760, Test Loss: 0.0638, Train Acc: 0.9750, Test Acc: 0.9667\n",
      "Epoch 79/100, Train Loss: 0.0758, Test Loss: 0.0640, Train Acc: 0.9750, Test Acc: 0.9667\n",
      "Epoch 80/100, Train Loss: 0.0757, Test Loss: 0.0641, Train Acc: 0.9750, Test Acc: 0.9667\n",
      "Epoch 81/100, Train Loss: 0.0755, Test Loss: 0.0643, Train Acc: 0.9750, Test Acc: 0.9667\n",
      "Epoch 82/100, Train Loss: 0.0754, Test Loss: 0.0644, Train Acc: 0.9750, Test Acc: 0.9667\n",
      "Epoch 83/100, Train Loss: 0.0753, Test Loss: 0.0646, Train Acc: 0.9750, Test Acc: 0.9667\n",
      "Epoch 84/100, Train Loss: 0.0751, Test Loss: 0.0647, Train Acc: 0.9750, Test Acc: 0.9667\n",
      "Epoch 85/100, Train Loss: 0.0750, Test Loss: 0.0649, Train Acc: 0.9750, Test Acc: 0.9667\n",
      "Epoch 86/100, Train Loss: 0.0749, Test Loss: 0.0650, Train Acc: 0.9750, Test Acc: 0.9667\n",
      "Epoch 87/100, Train Loss: 0.0747, Test Loss: 0.0652, Train Acc: 0.9750, Test Acc: 0.9667\n",
      "Epoch 88/100, Train Loss: 0.0746, Test Loss: 0.0653, Train Acc: 0.9750, Test Acc: 0.9667\n",
      "Epoch 89/100, Train Loss: 0.0745, Test Loss: 0.0654, Train Acc: 0.9750, Test Acc: 0.9667\n",
      "Epoch 90/100, Train Loss: 0.0744, Test Loss: 0.0656, Train Acc: 0.9750, Test Acc: 0.9667\n",
      "Epoch 91/100, Train Loss: 0.0742, Test Loss: 0.0657, Train Acc: 0.9750, Test Acc: 0.9667\n",
      "Epoch 92/100, Train Loss: 0.0741, Test Loss: 0.0659, Train Acc: 0.9750, Test Acc: 0.9667\n",
      "Epoch 93/100, Train Loss: 0.0740, Test Loss: 0.0660, Train Acc: 0.9750, Test Acc: 0.9667\n",
      "Epoch 94/100, Train Loss: 0.0739, Test Loss: 0.0661, Train Acc: 0.9750, Test Acc: 0.9667\n",
      "Epoch 95/100, Train Loss: 0.0738, Test Loss: 0.0663, Train Acc: 0.9750, Test Acc: 0.9667\n",
      "Epoch 96/100, Train Loss: 0.0737, Test Loss: 0.0664, Train Acc: 0.9750, Test Acc: 0.9667\n",
      "Epoch 97/100, Train Loss: 0.0736, Test Loss: 0.0665, Train Acc: 0.9750, Test Acc: 0.9667\n",
      "Epoch 98/100, Train Loss: 0.0735, Test Loss: 0.0666, Train Acc: 0.9750, Test Acc: 0.9667\n",
      "Epoch 99/100, Train Loss: 0.0734, Test Loss: 0.0668, Train Acc: 0.9750, Test Acc: 0.9667\n",
      "Epoch 100/100, Train Loss: 0.0732, Test Loss: 0.0669, Train Acc: 0.9750, Test Acc: 0.9667\n"
     ]
    }
   ],
   "source": [
    "mlp = MLP(input_dim, hidden_dim1, hidden_dim2, output_dim, k0_h1, k1_h1, k0_h2, k1_h2)\n",
    "\n",
    "# Set hyperparameters\n",
    "num_epochs = 100\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Train the model and get the evaluation metrics\n",
    "train_losses, test_losses, train_accs, test_accs,weight_updates = mlp.train(\n",
    "    X_train, y_train, X_test, y_test, num_epochs, learning_rate\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d73f251c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the test set\n",
    "y_pred = mlp.predict(X_test)\n",
    "\n",
    "# Convert one-hot encoded labels back to original labels\n",
    "y_test_original = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test_original, y_pred)\n",
    "\n",
    "# Calculate the F1-score\n",
    "f1_score_value = f1_score(y_test_original, y_pred, average='weighted')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d214040b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Parameter Values:\n",
      "Weight 1: [[-1.12227582e-01 -5.04136053e-01  2.04456023e+00 -1.98367302e+00\n",
      "  -6.07512108e-01  1.03841982e+00 -1.79393241e+00 -1.61402252e+00\n",
      "  -8.76402712e-01 -6.82653764e-01 -5.44430365e-01  1.63305570e+00\n",
      "   8.40899176e-01 -1.32131255e+00 -1.63064162e+00  1.44506603e-03]\n",
      " [ 7.80821689e-03 -1.19391000e+00 -4.17480967e-01 -5.32338223e-01\n",
      "   1.02533759e+00  2.38609974e-02 -5.50969675e-01 -4.72362009e-02\n",
      "  -1.64120198e+00 -6.68056973e-01 -6.71467333e-03 -2.10687554e+00\n",
      "   6.14337092e-01 -1.07987317e+00 -4.08365004e-01 -6.46592349e-01]\n",
      " [-7.08837898e-01 -7.31106008e-01 -1.93305113e+00 -6.71078162e-01\n",
      "   9.13256468e-01  2.45231323e-01 -2.60804622e+00  6.22603045e-01\n",
      "   2.59669651e-01  4.05810213e-01  9.53566344e-01 -1.38128814e-03\n",
      "   2.02035965e+00  1.53240720e+00 -9.30027956e-01  3.89330991e-01]\n",
      " [ 1.79807496e-01 -1.22265716e+00  1.02541367e+00  4.24992460e-01\n",
      "  -1.33719983e+00  1.01916791e+00  1.93898755e-01 -2.73774580e-01\n",
      "  -1.05056053e-01 -1.09966437e+00 -6.39616964e-01  1.58640805e+00\n",
      "  -2.35738921e+00 -5.46209311e-01  5.91330941e-01  5.40560825e-01]]\n",
      "Bias 1: [-0.3619391  -1.90166435 -1.13071976 -0.81583999  0.90682824 -1.07918614\n",
      " -1.49210842  0.22500749  0.44209684 -0.97728235 -0.44357085 -1.00535647\n",
      "  0.01107369 -0.87532247 -0.7999712   0.36955016]\n",
      "Weight 2: [[-4.68290417e-02  1.52071917e+00 -7.11330465e-02 -3.28673412e-01\n",
      "   1.29136881e+00 -1.45013421e+00  1.56837975e+00 -8.05480209e-01]\n",
      " [ 1.73419433e-01  8.82901527e-01 -1.24387024e+00 -1.10986677e+00\n",
      "   2.71971991e-01  1.45932915e+00 -6.19913093e-01 -8.10342696e-01]\n",
      " [ 2.25234966e+00 -1.28915870e+00  9.78105926e-01 -5.34133793e-01\n",
      "  -1.12388113e+00  9.12401289e-02 -7.97972954e-01 -4.61934778e-01]\n",
      " [-8.31476637e-01 -2.94785611e-01 -2.28879513e-01  1.50477786e+00\n",
      "   6.36529776e-01 -7.48678799e-01 -1.92149713e+00 -2.54495461e+00]\n",
      " [ 1.14542113e-01  5.40091163e-01  1.52660438e-01 -7.30355637e-01\n",
      "  -1.66354334e-01 -4.45910209e-01 -2.22502847e+00  2.59597089e-01]\n",
      " [-2.75919189e-01 -2.12836700e-01  3.79483330e-01 -1.46352198e-01\n",
      "   4.19303167e-01 -1.17359364e+00 -8.34459314e-01 -1.30379820e+00]\n",
      " [-2.06108619e+00 -1.34161687e+00 -8.15637863e-01 -9.43376485e-01\n",
      "   1.08262296e+00  6.85462517e-01 -1.74804863e-01 -4.17459290e-01]\n",
      " [-3.96667888e-01 -2.23564621e+00 -7.20380366e-01  3.48257160e-03\n",
      "  -9.58009871e-01  1.17274025e+00 -1.57215380e-01 -1.10494740e+00]\n",
      " [ 3.95161633e-01 -9.21074956e-01  1.70538534e+00 -5.04673347e-01\n",
      "   1.55755908e+00  1.85856530e-03  7.32190104e-01 -1.21640645e+00]\n",
      " [ 1.39116773e+00 -1.24877166e+00  1.02205609e+00  8.32134497e-01\n",
      "   3.38745024e-01 -1.36305279e-01 -9.58277207e-02  4.57273019e-01]\n",
      " [-4.04084922e-01  1.36818871e-01  7.86253491e-01 -1.30141743e+00\n",
      "   2.84534476e-01  4.58310251e-01 -1.28000359e-01  1.65056821e+00]\n",
      " [-1.94679253e+00  7.25097337e-01 -7.22442601e-01 -2.83245814e-01\n",
      "  -2.10611548e+00  4.47784024e-01  2.22454041e-02  1.74512023e+00]\n",
      " [ 9.85156973e-01 -1.85810689e-01 -4.36865745e-01  6.09811213e-01\n",
      "  -2.14935367e-01  1.21736329e+00  2.49257566e-03 -9.39307207e-01]\n",
      " [ 8.47223281e-01  1.00241388e+00 -3.49682519e-02  1.28179715e-01\n",
      "   1.40382076e-01 -5.64311359e-01  1.00303180e+00 -7.01526847e-01]\n",
      " [ 2.77483023e-01 -6.65721877e-01  3.00836522e-01 -4.46152332e-01\n",
      "  -2.13110411e-01 -1.93014362e+00  1.12680052e+00 -4.49763043e-02]\n",
      " [ 6.60327462e-02 -1.92276487e+00  6.97672174e-01  6.29334981e-01\n",
      "  -1.70712361e-01 -8.81594193e-01 -1.01516222e+00  3.89839848e-01]]\n",
      "Bias 2: [ 0.60644606 -0.06602809  1.33671505 -0.54004893  0.64722409 -1.03025852\n",
      " -0.4607758  -2.50322505]\n",
      "Weight 3: [[-0.75637102  0.65794745 -0.85431375]\n",
      " [ 0.14280159 -0.48514916  0.13897866]\n",
      " [-1.18708156  0.69926332  0.23299656]\n",
      " [ 1.75415862 -1.47623375  0.51854271]\n",
      " [ 0.18449729 -0.52420117 -0.24938856]\n",
      " [ 1.06002153 -1.28186584 -0.59023671]\n",
      " [-0.81544912 -0.77775859  0.12546583]\n",
      " [-1.05160702 -0.71584714 -0.01956184]]\n",
      "Bias 3: [-0.49252972 -0.36400587 -0.34932442]\n"
     ]
    }
   ],
   "source": [
    "#printing the values\n",
    "print(\"Final Parameter Values:\")\n",
    "print(\"Weight 1:\", mlp.w1)\n",
    "print(\"Bias 1:\", mlp.b1)\n",
    "print(\"Weight 2:\", mlp.w2)\n",
    "print(\"Bias 2:\", mlp.b2)\n",
    "print(\"Weight 3:\", mlp.w3)\n",
    "print(\"Bias 3:\", mlp.b3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ec3cba31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[10  0  0]\n",
      " [ 0  9  0]\n",
      " [ 0  1 10]]\n",
      "F1-score: 0.966750208855472\n"
     ]
    }
   ],
   "source": [
    "#for confusion Matrix\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"F1-score:\", f1_score_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4f60e198",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the loss function vs. epochs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(num_epochs), train_losses, label='Train Loss')\n",
    "plt.plot(range(num_epochs), test_losses, label='Test Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086102be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
